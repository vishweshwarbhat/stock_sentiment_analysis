{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pandas as pg\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  labels                                            message\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"SMSSpamCollection\",sep='\\t',names=['labels','message'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitizer=WordNetLemmatizer()\n",
    "corpus=[]\n",
    "for i in range(0,len(df.message)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[lemmitizer.lemmatize(word) for word in review if word not in stopwords.words(\"english\")]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import pandas as pd'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(corpus).toarray()\n",
    "y=pd.get_dummies(df['labels'])\n",
    "y=y.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[936  19]\n",
      " [  7 153]]\n",
      "Accuracy Score 0.9766816143497757\n",
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       955\n",
      "           1       0.89      0.96      0.92       160\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.94      0.97      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state = 0)\n",
    "spam_detect_model = MultinomialNB().fit(x_train, y_train)\n",
    "y_pred=spam_detect_model.predict(x_test)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\n",
    "print(\"Classification report: {}\".format(classification_report(y_test,y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm used LogisticRegression()\n",
      "[[955   0]\n",
      " [ 17 143]]\n",
      "algorithm used KNeighborsClassifier()\n",
      "[[955   0]\n",
      " [ 91  69]]\n",
      "algorithm used DecisionTreeClassifier()\n",
      "[[948   7]\n",
      " [ 16 144]]\n",
      "algorithm used RandomForestClassifier()\n",
      "[[955   0]\n",
      " [ 22 138]]\n",
      "[15:58:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "algorithm used XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "[[954   1]\n",
      " [ 13 147]]\n",
      "algorithm used MultinomialNB()\n",
      "[[936  19]\n",
      " [  7 153]]\n",
      "algorithm used GradientBoostingClassifier()\n",
      "[[953   2]\n",
      " [ 26 134]]\n"
     ]
    }
   ],
   "source": [
    "linear=LogisticRegression()\n",
    "knn=KNeighborsClassifier()\n",
    "dt=DecisionTreeClassifier()\n",
    "rf=RandomForestClassifier()\n",
    "xg=XGBClassifier()\n",
    "Mnb=MultinomialNB()\n",
    "gbc=GradientBoostingClassifier()\n",
    "algorithms=[linear,knn,dt,rf,xg,Mnb,gbc,]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    algorithm.fit(x_train,y_train)\n",
    "    y_pred=algorithm.predict(x_test)\n",
    "    print(\"algorithm used\"+\" \"+str(algorithm))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm used LogisticRegression()\n",
      "[[953   2]\n",
      " [ 39 121]]\n",
      "algorithm used KNeighborsClassifier()\n",
      "[[955   0]\n",
      " [ 98  62]]\n",
      "algorithm used DecisionTreeClassifier()\n",
      "[[933  22]\n",
      " [ 17 143]]\n",
      "algorithm used RandomForestClassifier()\n",
      "[[955   0]\n",
      " [ 19 141]]\n",
      "[16:08:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "algorithm used XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "[[951   4]\n",
      " [ 13 147]]\n",
      "algorithm used MultinomialNB()\n",
      "[[955   0]\n",
      " [ 31 129]]\n",
      "algorithm used GradientBoostingClassifier()\n",
      "[[955   0]\n",
      " [ 28 132]]\n"
     ]
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "x=tfidf.fit_transform(corpus).toarray()\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state = 0)\n",
    "linear=LogisticRegression()\n",
    "knn=KNeighborsClassifier()\n",
    "dt=DecisionTreeClassifier()\n",
    "rf=RandomForestClassifier()\n",
    "xg=XGBClassifier()\n",
    "Mnb=MultinomialNB()\n",
    "gbc=GradientBoostingClassifier()\n",
    "algorithms=[linear,knn,dt,rf,xg,Mnb,gbc,]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    algorithm.fit(x_train,y_train)\n",
    "    y_pred=algorithm.predict(x_test)\n",
    "    print(\"algorithm used\"+\" \"+str(algorithm))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem =PorterStemmer()\n",
    "corpus=[]\n",
    "for i in range(0,len(df.message)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['message'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[stem.stem(word) for word in review if word not in stopwords.words(\"english\")]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm used LogisticRegression()\n",
      "[[954   1]\n",
      " [ 17 143]]\n",
      "algorithm used KNeighborsClassifier()\n",
      "[[955   0]\n",
      " [ 91  69]]\n",
      "algorithm used DecisionTreeClassifier()\n",
      "[[949   6]\n",
      " [ 15 145]]\n",
      "algorithm used RandomForestClassifier()\n",
      "[[955   0]\n",
      " [ 20 140]]\n",
      "[16:18:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "algorithm used XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "[[953   2]\n",
      " [ 14 146]]\n",
      "algorithm used MultinomialNB()\n",
      "[[940  15]\n",
      " [  8 152]]\n",
      "algorithm used GradientBoostingClassifier()\n",
      "[[954   1]\n",
      " [ 29 131]]\n"
     ]
    }
   ],
   "source": [
    "cv=CountVectorizer()\n",
    "x=cv.fit_transform(corpus).toarray()\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state = 0)\n",
    "\n",
    "linear=LogisticRegression()\n",
    "knn=KNeighborsClassifier()\n",
    "dt=DecisionTreeClassifier()\n",
    "rf=RandomForestClassifier()\n",
    "xg=XGBClassifier()\n",
    "Mnb=MultinomialNB()\n",
    "gbc=GradientBoostingClassifier()\n",
    "algorithms=[linear,knn,dt,rf,xg,Mnb,gbc,]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    algorithm.fit(x_train,y_train)\n",
    "    y_pred=algorithm.predict(x_test)\n",
    "    print(\"algorithm used\"+\" \"+str(algorithm))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm used LogisticRegression()\n",
      "[[953   2]\n",
      " [ 39 121]]\n",
      "algorithm used KNeighborsClassifier()\n",
      "[[955   0]\n",
      " [ 97  63]]\n",
      "algorithm used DecisionTreeClassifier()\n",
      "[[939  16]\n",
      " [ 21 139]]\n",
      "algorithm used RandomForestClassifier()\n",
      "[[955   0]\n",
      " [ 19 141]]\n",
      "[16:26:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "algorithm used XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
      "              importance_type='gain', interaction_constraints='',\n",
      "              learning_rate=0.300000012, max_delta_step=0, max_depth=6,\n",
      "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "              n_estimators=100, n_jobs=8, num_parallel_tree=1, random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n",
      "              tree_method='exact', validate_parameters=1, verbosity=None)\n",
      "[[949   6]\n",
      " [ 15 145]]\n",
      "algorithm used MultinomialNB()\n",
      "[[955   0]\n",
      " [ 34 126]]\n",
      "algorithm used GradientBoostingClassifier()\n",
      "[[952   3]\n",
      " [ 29 131]]\n"
     ]
    }
   ],
   "source": [
    "tfidf=TfidfVectorizer()\n",
    "x=tfidf.fit_transform(corpus).toarray()\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state = 0)\n",
    "linear=LogisticRegression()\n",
    "knn=KNeighborsClassifier()\n",
    "dt=DecisionTreeClassifier()\n",
    "rf=RandomForestClassifier()\n",
    "xg=XGBClassifier()\n",
    "Mnb=MultinomialNB()\n",
    "gbc=GradientBoostingClassifier()\n",
    "algorithms=[linear,knn,dt,rf,xg,Mnb,gbc,]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    algorithm.fit(x_train,y_train)\n",
    "    y_pred=algorithm.predict(x_test)\n",
    "    print(\"algorithm used\"+\" \"+str(algorithm))\n",
    "    print(confusion_matrix(y_test,y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## best performing algorithm is XGBC\n",
    "* both stemming and lemmatixation is giving same accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------THE END--------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
